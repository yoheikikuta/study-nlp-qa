{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is constructed from https://github.com/Kyubyong/transformer for my study."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-                   100%[===================>]  23.06M   322KB/s    in 2m 23s  \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "if not os.path.exists(\"./corpora\"):\n",
    "    os.makedirs(\"./corpora\", exist_ok=True)\n",
    "    !wget -qO- --show-progress https://wit3.fbk.eu/archive/2016-01//texts/de/en/de-en.tgz | tar xz; mv de-en corpora"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Hyperparams:\n",
    "    '''Hyperparameters'''\n",
    "    # data\n",
    "    source_train = 'corpora/de-en/train.tags.de-en.de'\n",
    "    target_train = 'corpora/de-en/train.tags.de-en.en'\n",
    "    source_test = 'corpora/de-en/IWSLT16.TED.tst2014.de-en.de.xml'\n",
    "    target_test = 'corpora/de-en/IWSLT16.TED.tst2014.de-en.en.xml'\n",
    "    \n",
    "    # training\n",
    "    batch_size = 32 # alias = N\n",
    "    lr = 0.0001 # learning rate. In paper, learning rate is adjusted to the global step.\n",
    "    logdir = 'logdir' # log directory\n",
    "    \n",
    "    # model\n",
    "    maxlen = 10 # Maximum number of words in a sentence. alias = T.  Feel free to increase this if you are ambitious.\n",
    "    min_cnt = 20 # words whose occurred less than min_cnt are encoded as <UNK>.\n",
    "    hidden_units = 512 # alias = C\n",
    "    num_blocks = 6 # number of encoder/decoder blocks\n",
    "    num_epochs = 20\n",
    "    num_heads = 8\n",
    "    dropout_rate = 0.1\n",
    "    sinusoid = False # If True, use sinusoid. If false, positional embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "hp = Hyperparams()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define functions for data handling and modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import codecs\n",
    "import os\n",
    "import regex\n",
    "from collections import Counter\n",
    "\n",
    "def make_vocab(fpath\n",
    "               , fname):\n",
    "    '''Constructs vocabulary.\n",
    "    \n",
    "    Args:\n",
    "      fpath: A string. Input file path.\n",
    "      fname: A string. Output file name.\n",
    "    \n",
    "    Writes vocabulary line by line to `preprocessed/fname`\n",
    "    '''  \n",
    "    text = codecs.open(fpath, 'r', 'utf-8').read()\n",
    "    text = regex.sub(\"[^\\s\\p{Latin}']\", \"\", text)\n",
    "    words = text.split()\n",
    "    word2cnt = Counter(words)\n",
    "    \n",
    "    if not os.path.exists('preprocessed'): os.mkdir('preprocessed')\n",
    "    \n",
    "    with codecs.open('preprocessed/{}'.format(fname), 'w', 'utf-8') as fout:\n",
    "        fout.write(\"{}\\t1000000000\\n{}\\t1000000000\\n{}\\t1000000000\\n{}\\t1000000000\\n\".format(\"<PAD>\", \"<UNK>\", \"<S>\", \"</S>\"))\n",
    "        for word, cnt in word2cnt.most_common(len(word2cnt)):\n",
    "            fout.write(u\"{}\\t{}\\n\".format(word, cnt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "CPU times: user 3.46 s, sys: 310 ms, total: 3.77 s\n",
      "Wall time: 3.94 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "if not os.path.exists(\"./preprocessed/de.vocab.tsv\"):\n",
    "    make_vocab(hp.source_train, \"de.vocab.tsv\")\n",
    "    make_vocab(hp.target_train, \"en.vocab.tsv\")\n",
    "    print(\"Done\")\n",
    "else:\n",
    "    print(\"File already exists.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(inputs\n",
    "              , epsilon = 1e-8\n",
    "              , scope=\"ln\"\n",
    "              , reuse=None):\n",
    "    '''Applies layer normalization.\n",
    "    \n",
    "    Args:\n",
    "      inputs: A tensor with 2 or more dimensions, where the first dimension has `batch_size`.\n",
    "      epsilon: A floating number. A very small number for preventing ZeroDivision Error.\n",
    "      scope: Optional scope for `variable_scope`.\n",
    "      reuse: Boolean, whether to reuse the weights of a previous layer by the same name.\n",
    "      \n",
    "    Returns:\n",
    "      A tensor with the same shape and data dtype as `inputs`.\n",
    "    '''\n",
    "    with tf.variable_scope(scope, reuse=reuse):\n",
    "        inputs_shape = inputs.get_shape()\n",
    "        params_shape = inputs_shape[-1:]\n",
    "    \n",
    "        mean, variance = tf.nn.moments(inputs, [-1], keep_dims=True)\n",
    "        beta= tf.Variable(tf.zeros(params_shape)) #Is this necessary?\n",
    "        gamma = tf.Variable(tf.ones(params_shape))\n",
    "        normalized = (inputs - mean) / ( (variance + epsilon) ** (.5) )\n",
    "        outputs = gamma * normalized + beta\n",
    "        \n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.2247448   0.          1.2247448 ]\n",
      " [-1.0690448  -0.26726115  1.3363062 ]]\n"
     ]
    }
   ],
   "source": [
    "inputs = tf.constant([[1.0, 2.0, 3.0],[4.0, 8.0, 16.0]], name=\"test\")\n",
    "outputs = normalize(inputs, scope='test')\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    print(sess.run(outputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embedding(inputs\n",
    "              , vocab_size\n",
    "              , num_units\n",
    "              , zero_pad=True\n",
    "              , scale=True\n",
    "              , scope=\"embedding\"\n",
    "              , reuse=None):\n",
    "    '''Embeds a given tensor.\n",
    "    Args:\n",
    "      inputs: A `Tensor` with type `int32` or `int64` containing the ids to be looked up in `lookup table`.\n",
    "      vocab_size: An int. Vocabulary size.\n",
    "      num_units: An int. Number of embedding hidden units.\n",
    "      zero_pad: A boolean. If True, all the values of the fist row (id 0) should be constant zeros.\n",
    "      scale: A boolean. If True. the outputs is multiplied by sqrt num_units.\n",
    "      scope: Optional scope for `variable_scope`.\n",
    "      reuse: Boolean, whether to reuse the weights of a previous layer by the same name.\n",
    "    Returns:\n",
    "      A `Tensor` with one more rank than inputs's. The last dimensionality should be `num_units`.\n",
    "        \n",
    "    For example,\n",
    "    \n",
    "    ```\n",
    "    import tensorflow as tf\n",
    "    \n",
    "    inputs = tf.to_int32(tf.reshape(tf.range(2*3), (2, 3)))\n",
    "    outputs = embedding(inputs, 6, 2, zero_pad=True)\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        print sess.run(outputs)\n",
    "    >>\n",
    "    [[[ 0.          0.        ]\n",
    "      [ 0.09754146  0.67385566]\n",
    "      [ 0.37864095 -0.35689294]]\n",
    "     [[-1.01329422 -1.09939694]\n",
    "      [ 0.7521342   0.38203377]\n",
    "      [-0.04973143 -0.06210355]]]\n",
    "    ```\n",
    "    \n",
    "    ```\n",
    "    import tensorflow as tf\n",
    "    \n",
    "    inputs = tf.to_int32(tf.reshape(tf.range(2*3), (2, 3)))\n",
    "    outputs = embedding(inputs, 6, 2, zero_pad=False)\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        print sess.run(outputs)\n",
    "    >>\n",
    "    [[[-0.19172323 -0.39159766]\n",
    "      [-0.43212751 -0.66207761]\n",
    "      [ 1.03452027 -0.26704335]]\n",
    "     [[-0.11634696 -0.35983452]\n",
    "      [ 0.50208133  0.53509563]\n",
    "      [ 1.22204471 -0.96587461]]]    \n",
    "    ```    \n",
    "    '''\n",
    "    with tf.variable_scope(scope, reuse=reuse):\n",
    "        lookup_table = tf.get_variable('lookup_table'\n",
    "                                       , dtype=tf.float32\n",
    "                                       , shape=[vocab_size, num_units]\n",
    "                                       , initializer=tf.contrib.layers.xavier_initializer())\n",
    "        if zero_pad:\n",
    "            lookup_table = tf.concat((tf.zeros(shape=[1, num_units]), lookup_table[1:, :]), 0)\n",
    "        outputs = tf.nn.embedding_lookup(lookup_table, inputs)\n",
    "        \n",
    "        if scale:\n",
    "            outputs = outputs * (num_units ** 0.5) \n",
    "            \n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[-0.26374754 -1.2154895 ]\n",
      "  [-0.85820097  0.0764838 ]\n",
      "  [-0.6088722  -1.0595982 ]]\n",
      "\n",
      " [[-0.06707426  0.9858819 ]\n",
      "  [-0.09764834  1.02891   ]\n",
      "  [-1.0831312  -1.04858   ]]]\n"
     ]
    }
   ],
   "source": [
    "inputs = tf.to_int32(tf.reshape(tf.range(2*3), (2, 3)))\n",
    "outputs = embedding(inputs, 6, 2, zero_pad=False)\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    print( sess.run(outputs) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Reshape:0' shape=(2, 3) dtype=int32>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Tensor.get_shape of <tf.Tensor 'embedding/mul:0' shape=(2, 3, 2) dtype=float32>>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.get_shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last dimension 2 is for num_units."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encoding(inputs\n",
    "                        , num_units\n",
    "                        , zero_pad=True\n",
    "                        , scale=True\n",
    "                        , scope=\"positional_encoding\"\n",
    "                        , reuse=None):\n",
    "    '''Sinusoidal Positional_Encoding.\n",
    "    Args:\n",
    "      inputs: A 2d Tensor with shape of (N, T).\n",
    "      num_units: Output dimensionality\n",
    "      zero_pad: Boolean. If True, all the values of the first row (id = 0) should be constant zero\n",
    "      scale: Boolean. If True, the output will be multiplied by sqrt num_units(check details from paper)\n",
    "      scope: Optional scope for `variable_scope`.\n",
    "      reuse: Boolean, whether to reuse the weights of a previous layer by the same name.\n",
    "    Returns:\n",
    "        A 'Tensor' with one more rank than inputs's, with the dimensionality should be 'num_units'\n",
    "    '''\n",
    "\n",
    "    N, T = inputs.get_shape().as_list()\n",
    "    with tf.variable_scope(scope, reuse=reuse):\n",
    "        position_ind = tf.tile(tf.expand_dims(tf.range(T), 0), [N, 1])\n",
    "\n",
    "        # First part of the PE function: sin and cos argument\n",
    "        position_enc = np.array([ [pos / np.power(10000, 2.*i/num_units) for i in range(num_units)] for pos in range(T) ])\n",
    "\n",
    "        # Second part, apply the cosine to even columns and sin to odds.\n",
    "        position_enc[:, 0::2] = np.sin(position_enc[:, 0::2])  # dim 2i\n",
    "        position_enc[:, 1::2] = np.cos(position_enc[:, 1::2])  # dim 2i+1\n",
    "\n",
    "        # Convert to a tensor\n",
    "        lookup_table = tf.convert_to_tensor(position_enc)\n",
    "\n",
    "        if zero_pad:\n",
    "            lookup_table = tf.concat((tf.zeros(shape=[1, num_units]), lookup_table[1:, :]), 0)\n",
    "        outputs = tf.nn.embedding_lookup(lookup_table, position_ind)\n",
    "\n",
    "        if scale:\n",
    "            outputs = outputs * num_units**0.5\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0.         1.41421356]\n",
      "  [1.19001968 1.41421356]\n",
      "  [1.28594075 1.41421353]\n",
      "  [0.19957383 1.4142135 ]]\n",
      "\n",
      " [[0.         1.41421356]\n",
      "  [1.19001968 1.41421356]\n",
      "  [1.28594075 1.41421353]\n",
      "  [0.19957383 1.4142135 ]]\n",
      "\n",
      " [[0.         1.41421356]\n",
      "  [1.19001968 1.41421356]\n",
      "  [1.28594075 1.41421353]\n",
      "  [0.19957383 1.4142135 ]]]\n"
     ]
    }
   ],
   "source": [
    "inputs = tf.constant([[0, 0, 0, 0],[0, 1, 0, 0],[0, 0, 1, 0]])\n",
    "outputs = positional_encoding(inputs, 2, zero_pad=False)\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    print( sess.run(outputs) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Tensor.get_shape of <tf.Tensor 'Const:0' shape=(3, 4) dtype=int32>>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.get_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.e+00, 0.e+00, 0.e+00, 0.e+00],\n",
       "       [1.e+00, 1.e-02, 1.e-04, 1.e-06],\n",
       "       [2.e+00, 2.e-02, 2.e-04, 2.e-06]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([ [pos / np.power(10000, 2.*i/4) for i in range(4)] for pos in range(3) ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multihead_attention(queries, \n",
    "                        keys, \n",
    "                        num_units=None, \n",
    "                        num_heads=8, \n",
    "                        dropout_rate=0,\n",
    "                        is_training=True,\n",
    "                        causality=False,\n",
    "                        scope=\"multihead_attention\", \n",
    "                        reuse=None):\n",
    "    '''Applies multihead attention.\n",
    "    \n",
    "    Args:\n",
    "      queries: A 3d tensor with shape of [N, T_q, C_q].\n",
    "      keys: A 3d tensor with shape of [N, T_k, C_k].\n",
    "      num_units: A scalar. Attention size.\n",
    "      dropout_rate: A floating point number.\n",
    "      is_training: Boolean. Controller of mechanism for dropout.\n",
    "      causality: Boolean. If true, units that reference the future are masked. \n",
    "      num_heads: An int. Number of heads.\n",
    "      scope: Optional scope for `variable_scope`.\n",
    "      reuse: Boolean, whether to reuse the weights of a previous layer by the same name.\n",
    "        \n",
    "    Returns\n",
    "      A 3d tensor with shape of (N, T_q, C)  \n",
    "    '''\n",
    "    with tf.variable_scope(scope, reuse=reuse):\n",
    "        # Set the fall back option for num_units\n",
    "        if num_units is None:\n",
    "            num_units = queries.get_shape().as_list[-1]\n",
    "        \n",
    "        # Linear projections\n",
    "        Q = tf.layers.dense(queries, num_units, activation=tf.nn.relu) # (N, T_q, C)\n",
    "        K = tf.layers.dense(keys, num_units, activation=tf.nn.relu) # (N, T_k, C)\n",
    "        V = tf.layers.dense(keys, num_units, activation=tf.nn.relu) # (N, T_k, C)\n",
    "        \n",
    "        # Split and concat\n",
    "        Q_ = tf.concat(tf.split(Q, num_heads, axis=2), axis=0) # (h*N, T_q, C/h) \n",
    "        K_ = tf.concat(tf.split(K, num_heads, axis=2), axis=0) # (h*N, T_k, C/h) \n",
    "        V_ = tf.concat(tf.split(V, num_heads, axis=2), axis=0) # (h*N, T_k, C/h) \n",
    "\n",
    "        # Multiplication\n",
    "        outputs = tf.matmul(Q_, tf.transpose(K_, [0, 2, 1])) # (h*N, T_q, T_k) < Q_ * K_^T\n",
    "        \n",
    "        # Scale\n",
    "        outputs = outputs / (K_.get_shape().as_list()[-1] ** 0.5)\n",
    "        \n",
    "        # Key Masking\n",
    "        key_masks = tf.sign(tf.abs(tf.reduce_sum(keys, axis=-1))) # (N, T_k) < get sign < compute abs < sum over hidden units\n",
    "        key_masks = tf.tile(key_masks, [num_heads, 1]) # (h*N, T_k)\n",
    "        key_masks = tf.tile(tf.expand_dims(key_masks, 1), [1, tf.shape(queries)[1], 1]) # (h*N, T_q, T_k)\n",
    "        \n",
    "        paddings = tf.ones_like(outputs)*(-2**32+1)\n",
    "        outputs = tf.where(tf.equal(key_masks, 0), paddings, outputs) # (h*N, T_q, T_k)\n",
    "  \n",
    "        # Causality = Future blinding\n",
    "        if causality:\n",
    "            diag_vals = tf.ones_like(outputs[0, :, :]) # (T_q, T_k)\n",
    "            tril = tf.contrib.linalg.LinearOperatorTriL(diag_vals).to_dense() # (T_q, T_k) < LinearOperator acting like a [batch] square lower triangular matrix\n",
    "            masks = tf.tile(tf.expand_dims(tril, 0), [tf.shape(outputs)[0], 1, 1]) # (h*N, T_q, T_k)\n",
    "   \n",
    "            paddings = tf.ones_like(masks)*(-2**32+1)\n",
    "            outputs = tf.where(tf.equal(masks, 0), paddings, outputs) # (h*N, T_q, T_k)\n",
    "  \n",
    "        # Activation\n",
    "        outputs = tf.nn.softmax(outputs) # (h*N, T_q, T_k)\n",
    "         \n",
    "        # Query Masking\n",
    "        query_masks = tf.sign(tf.abs(tf.reduce_sum(queries, axis=-1))) # (N, T_q)\n",
    "        query_masks = tf.tile(query_masks, [num_heads, 1]) # (h*N, T_q)\n",
    "        query_masks = tf.tile(tf.expand_dims(query_masks, -1), [1, 1, tf.shape(keys)[1]]) # (h*N, T_q, T_k)\n",
    "        outputs *= query_masks # broadcasting. (N, T_q, C)\n",
    "          \n",
    "        # Dropouts\n",
    "        outputs = tf.layers.dropout(outputs, rate=dropout_rate, training=tf.convert_to_tensor(is_training))\n",
    "               \n",
    "        # Weighted sum\n",
    "        outputs = tf.matmul(outputs, V_) # ( h*N, T_q, C/h) < masked(Q_ * K_^T) * V_\n",
    "        \n",
    "        # Restore shape\n",
    "        outputs = tf.concat( tf.split(outputs, num_heads, axis=0), axis=2 ) # (N, T_q, C)\n",
    "              \n",
    "        # Residual connection\n",
    "        outputs += queries\n",
    "              \n",
    "        # Normalize\n",
    "        outputs = normalize(outputs) # (N, T_q, C)\n",
    " \n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feedforward(inputs\n",
    "                , num_units=[2048, 512] #Two hidden layers\n",
    "                , scope=\"multihead_attention\"\n",
    "                , reuse=None):\n",
    "    '''Point-wise feed forward net.\n",
    "    \n",
    "    Args:\n",
    "      inputs: A 3d tensor with shape of [N, T, C].\n",
    "      num_units: A list of two integers.\n",
    "      scope: Optional scope for `variable_scope`.\n",
    "      reuse: Boolean, whether to reuse the weights of a previous layer by the same name.\n",
    "        \n",
    "    Returns:\n",
    "      A 3d tensor with the same shape and dtype as inputs\n",
    "    '''\n",
    "    with tf.variable_scope(scope, reuse=reuse):\n",
    "        # Inner layer\n",
    "        params = {\"inputs\": inputs, \"filters\": num_units[0], \"kernel_size\": 1,\n",
    "                  \"activation\": tf.nn.relu, \"use_bias\": True}\n",
    "        outputs = tf.layers.conv1d(**params)\n",
    "        \n",
    "        # Readout layer\n",
    "        params = {\"inputs\": outputs, \"filters\": num_units[1], \"kernel_size\": 1,\n",
    "                  \"activation\": None, \"use_bias\": True}\n",
    "        outputs = tf.layers.conv1d(**params)\n",
    "        \n",
    "        # Residual connection\n",
    "        outputs += inputs\n",
    "        \n",
    "        # Normalize\n",
    "        outputs = normalize(outputs)\n",
    "    \n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_smoothing(inputs\n",
    "                    , epsilon=0.1):\n",
    "    '''Applies label smoothing. See https://arxiv.org/abs/1512.00567.\n",
    "    \n",
    "    Args:\n",
    "      inputs: A 3d tensor with shape of [N, T, V], where V is the number of vocabulary.\n",
    "      epsilon: Smoothing rate.\n",
    "    \n",
    "    For example,\n",
    "    \n",
    "    ```\n",
    "    import tensorflow as tf\n",
    "    inputs = tf.convert_to_tensor([[[0, 0, 1], \n",
    "       [0, 1, 0],\n",
    "       [1, 0, 0]],\n",
    "      [[1, 0, 0],\n",
    "       [1, 0, 0],\n",
    "       [0, 1, 0]]], tf.float32)\n",
    "       \n",
    "    outputs = label_smoothing(inputs)\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        print(sess.run([outputs]))\n",
    "    \n",
    "    >>\n",
    "    [array([[[ 0.03333334,  0.03333334,  0.93333334],\n",
    "        [ 0.03333334,  0.93333334,  0.03333334],\n",
    "        [ 0.93333334,  0.03333334,  0.03333334]],\n",
    "       [[ 0.93333334,  0.03333334,  0.03333334],\n",
    "        [ 0.93333334,  0.03333334,  0.03333334],\n",
    "        [ 0.03333334,  0.93333334,  0.03333334]]], dtype=float32)]   \n",
    "    ```    \n",
    "    '''\n",
    "    K = inputs.get_shape().as_list()[-1] # number of channels\n",
    "    return ((1-epsilon) * inputs) + (epsilon / K)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_de_vocab():\n",
    "    vocab = [line.split()[0] for line in codecs.open('preprocessed/de.vocab.tsv', 'r', 'utf-8').read().splitlines() if int(line.split()[1])>=hp.min_cnt]\n",
    "    word2idx = {word: idx for idx, word in enumerate(vocab)}\n",
    "    idx2word = {idx: word for idx, word in enumerate(vocab)}\n",
    "    return word2idx, idx2word\n",
    "\n",
    "def load_en_vocab():\n",
    "    vocab = [line.split()[0] for line in codecs.open('preprocessed/en.vocab.tsv', 'r', 'utf-8').read().splitlines() if int(line.split()[1])>=hp.min_cnt]\n",
    "    word2idx = {word: idx for idx, word in enumerate(vocab)}\n",
    "    idx2word = {idx: word for idx, word in enumerate(vocab)}\n",
    "    return word2idx, idx2word\n",
    "\n",
    "def create_data(source_sents, target_sents): \n",
    "    de2idx, idx2de = load_de_vocab()\n",
    "    en2idx, idx2en = load_en_vocab()\n",
    "    \n",
    "    # Index\n",
    "    x_list, y_list, Sources, Targets = [], [], [], []\n",
    "    for source_sent, target_sent in zip(source_sents, target_sents):\n",
    "        x = [de2idx.get(word, 1) for word in (source_sent + u\" </S>\").split()] # 1: OOV, </S>: End of Text\n",
    "        y = [en2idx.get(word, 1) for word in (target_sent + u\" </S>\").split()] \n",
    "        if max(len(x), len(y)) <=hp.maxlen:\n",
    "            x_list.append(np.array(x))\n",
    "            y_list.append(np.array(y))\n",
    "            Sources.append(source_sent)\n",
    "            Targets.append(target_sent)\n",
    "    \n",
    "    # Pad      \n",
    "    X = np.zeros([len(x_list), hp.maxlen], np.int32)\n",
    "    Y = np.zeros([len(y_list), hp.maxlen], np.int32)\n",
    "    for i, (x, y) in enumerate(zip(x_list, y_list)):\n",
    "        X[i] = np.lib.pad(x, [0, hp.maxlen-len(x)], 'constant', constant_values=(0, 0))\n",
    "        Y[i] = np.lib.pad(y, [0, hp.maxlen-len(y)], 'constant', constant_values=(0, 0))\n",
    "    \n",
    "    return X, Y, Sources, Targets\n",
    "\n",
    "def load_train_data():\n",
    "    de_sents = [regex.sub(\"[^\\s\\p{Latin}']\", \"\", line) for line in codecs.open(hp.source_train, 'r', 'utf-8').read().split(\"\\n\") if line and line[0] != \"<\"]\n",
    "    en_sents = [regex.sub(\"[^\\s\\p{Latin}']\", \"\", line) for line in codecs.open(hp.target_train, 'r', 'utf-8').read().split(\"\\n\") if line and line[0] != \"<\"]\n",
    "    \n",
    "    X, Y, Sources, Targets = create_data(de_sents, en_sents)\n",
    "    return X, Y\n",
    "    \n",
    "def load_test_data():\n",
    "    def _refine(line):\n",
    "        line = regex.sub(\"<[^>]+>\", \"\", line)\n",
    "        line = regex.sub(\"[^\\s\\p{Latin}']\", \"\", line) \n",
    "        return line.strip()\n",
    "    \n",
    "    de_sents = [_refine(line) for line in codecs.open(hp.source_test, 'r', 'utf-8').read().split(\"\\n\") if line and line[:4] == \"<seg\"]\n",
    "    en_sents = [_refine(line) for line in codecs.open(hp.target_test, 'r', 'utf-8').read().split(\"\\n\") if line and line[:4] == \"<seg\"]\n",
    "        \n",
    "    X, Y, Sources, Targets = create_data(de_sents, en_sents)\n",
    "    return X, Sources, Targets # (1064, 150)\n",
    "\n",
    "def get_batch_data():\n",
    "    # Load data\n",
    "    X, Y = load_train_data()\n",
    "    \n",
    "    # calc total batch count\n",
    "    num_batch = len(X) // hp.batch_size\n",
    "    \n",
    "    # Convert to tensor\n",
    "    X = tf.convert_to_tensor(X, tf.int32)\n",
    "    Y = tf.convert_to_tensor(Y, tf.int32)\n",
    "    \n",
    "    # Create Queues\n",
    "    input_queues = tf.train.slice_input_producer([X, Y])\n",
    "            \n",
    "    # create batch queues\n",
    "    x, y = tf.train.shuffle_batch(input_queues,\n",
    "                                num_threads=8,\n",
    "                                batch_size=hp.batch_size, \n",
    "                                capacity=hp.batch_size*64,   \n",
    "                                min_after_dequeue=hp.batch_size*32, \n",
    "                                allow_smaller_final_batch=False)\n",
    "    \n",
    "    return x, y, num_batch # (N, T), (N, T), ()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define computational graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "\n",
    "class Graph():\n",
    "    def __init__(self, is_training=True):\n",
    "        self.graph = tf.Graph()\n",
    "        with self.graph.as_default():\n",
    "            if is_training:\n",
    "                self.x, self.y, self.num_batch = get_batch_data() # (N, T)\n",
    "            else: # inference\n",
    "                self.x = tf.placeholder(tf.int32, shape=(None, hp.maxlen))\n",
    "                self.y = tf.placeholder(tf.int32, shape=(None, hp.maxlen))\n",
    "\n",
    "            # define decoder inputs\n",
    "            self.decoder_inputs = tf.concat((tf.ones_like(self.y[:, :1])*2, self.y[:, :-1]), -1) # 2:<S>\n",
    "\n",
    "            # Load vocabulary    \n",
    "            de2idx, idx2de = load_de_vocab()\n",
    "            en2idx, idx2en = load_en_vocab()\n",
    "            \n",
    "            # Encoder\n",
    "            with tf.variable_scope(\"encoder\"):\n",
    "                ## Embedding\n",
    "                self.enc = embedding(self.x, \n",
    "                                      vocab_size=len(de2idx), \n",
    "                                      num_units=hp.hidden_units, \n",
    "                                      scale=True,\n",
    "                                      scope=\"enc_embed\")\n",
    "                \n",
    "                ## Positional Encoding\n",
    "                if hp.sinusoid:\n",
    "                    self.enc += positional_encoding(self.x,\n",
    "                                      num_units=hp.hidden_units, \n",
    "                                      zero_pad=False, \n",
    "                                      scale=False,\n",
    "                                      scope=\"enc_pe\")\n",
    "                else:\n",
    "                    self.enc += embedding(tf.tile(tf.expand_dims(tf.range(tf.shape(self.x)[1]), 0), [tf.shape(self.x)[0], 1]),\n",
    "                                      vocab_size=hp.maxlen, \n",
    "                                      num_units=hp.hidden_units, \n",
    "                                      zero_pad=False, \n",
    "                                      scale=False,\n",
    "                                      scope=\"enc_pe\")\n",
    "                    \n",
    "                 \n",
    "                ## Dropout\n",
    "                self.enc = tf.layers.dropout(self.enc, \n",
    "                                            rate=hp.dropout_rate, \n",
    "                                            training=tf.convert_to_tensor(is_training))\n",
    "                \n",
    "                ## Blocks\n",
    "                for i in range(hp.num_blocks):\n",
    "                    with tf.variable_scope(\"num_blocks_{}\".format(i)):\n",
    "                        ### Multihead Attention\n",
    "                        self.enc = multihead_attention(queries=self.enc, \n",
    "                                                        keys=self.enc, \n",
    "                                                        num_units=hp.hidden_units, \n",
    "                                                        num_heads=hp.num_heads, \n",
    "                                                        dropout_rate=hp.dropout_rate,\n",
    "                                                        is_training=is_training,\n",
    "                                                        causality=False)\n",
    "                        \n",
    "                        ### Feed Forward\n",
    "                        self.enc = feedforward(self.enc, num_units=[4*hp.hidden_units, hp.hidden_units])\n",
    "            \n",
    "            # Decoder\n",
    "            with tf.variable_scope(\"decoder\"):\n",
    "                ## Embedding\n",
    "                self.dec = embedding(self.decoder_inputs, \n",
    "                                      vocab_size=len(en2idx), \n",
    "                                      num_units=hp.hidden_units,\n",
    "                                      scale=True, \n",
    "                                      scope=\"dec_embed\")\n",
    "                \n",
    "                ## Positional Encoding\n",
    "                if hp.sinusoid:\n",
    "                    self.dec += positional_encoding(self.decoder_inputs,\n",
    "                                      vocab_size=hp.maxlen, \n",
    "                                      num_units=hp.hidden_units, \n",
    "                                      zero_pad=False, \n",
    "                                      scale=False,\n",
    "                                      scope=\"dec_pe\")\n",
    "                else:\n",
    "                    self.dec += embedding(tf.tile(tf.expand_dims(tf.range(tf.shape(self.decoder_inputs)[1]), 0), [tf.shape(self.decoder_inputs)[0], 1]),\n",
    "                                      vocab_size=hp.maxlen, \n",
    "                                      num_units=hp.hidden_units, \n",
    "                                      zero_pad=False, \n",
    "                                      scale=False,\n",
    "                                      scope=\"dec_pe\")\n",
    "                \n",
    "                ## Dropout\n",
    "                self.dec = tf.layers.dropout(self.dec, \n",
    "                                            rate=hp.dropout_rate, \n",
    "                                            training=tf.convert_to_tensor(is_training))\n",
    "                \n",
    "                ## Blocks\n",
    "                for i in range(hp.num_blocks):\n",
    "                    with tf.variable_scope(\"num_blocks_{}\".format(i)):\n",
    "                        ## Multihead Attention ( self-attention)\n",
    "                        self.dec = multihead_attention(queries=self.dec, \n",
    "                                                        keys=self.dec, \n",
    "                                                        num_units=hp.hidden_units, \n",
    "                                                        num_heads=hp.num_heads, \n",
    "                                                        dropout_rate=hp.dropout_rate,\n",
    "                                                        is_training=is_training,\n",
    "                                                        causality=True, \n",
    "                                                        scope=\"self_attention\")\n",
    "                        \n",
    "                        ## Multihead Attention ( vanilla attention)\n",
    "                        self.dec = multihead_attention(queries=self.dec, \n",
    "                                                        keys=self.enc, \n",
    "                                                        num_units=hp.hidden_units, \n",
    "                                                        num_heads=hp.num_heads,\n",
    "                                                        dropout_rate=hp.dropout_rate,\n",
    "                                                        is_training=is_training, \n",
    "                                                        causality=False,\n",
    "                                                        scope=\"vanilla_attention\")\n",
    "                        \n",
    "                        ## Feed Forward\n",
    "                        self.dec = feedforward(self.dec, num_units=[4*hp.hidden_units, hp.hidden_units])\n",
    "                \n",
    "            # Final linear projection\n",
    "            self.logits = tf.layers.dense(self.dec, len(en2idx))\n",
    "            self.preds = tf.to_int32(tf.arg_max(self.logits, dimension=-1))\n",
    "            self.istarget = tf.to_float(tf.not_equal(self.y, 0))\n",
    "            self.acc = tf.reduce_sum(tf.to_float(tf.equal(self.preds, self.y))*self.istarget)/ (tf.reduce_sum(self.istarget))\n",
    "            tf.summary.scalar('acc', self.acc)\n",
    "                \n",
    "            if is_training:  \n",
    "                # Loss\n",
    "                self.y_smoothed = label_smoothing(tf.one_hot(self.y, depth=len(en2idx)))\n",
    "                self.loss = tf.nn.softmax_cross_entropy_with_logits(logits=self.logits, labels=self.y_smoothed)\n",
    "                self.mean_loss = tf.reduce_sum(self.loss*self.istarget) / (tf.reduce_sum(self.istarget))\n",
    "               \n",
    "                # Training Scheme\n",
    "                self.global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "                self.optimizer = tf.train.AdamOptimizer(learning_rate=hp.lr, beta1=0.9, beta2=0.98, epsilon=1e-8)\n",
    "                self.train_op = self.optimizer.minimize(self.mean_loss, global_step=self.global_step)\n",
    "                   \n",
    "                # Summary \n",
    "                tf.summary.scalar('mean_loss', self.mean_loss)\n",
    "                self.merged = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load vocabulary    \n",
    "# de2idx, idx2de = load_de_vocab()\n",
    "# en2idx, idx2en = load_en_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct graph\n",
    "g = Graph(\"train\"); print(\"Graph loaded\")\n",
    "\n",
    "# Start session\n",
    "sv = tf.train.Supervisor(graph=g.graph, \n",
    "                         logdir=hp.logdir,\n",
    "                         save_model_secs=0)\n",
    "\n",
    "with sv.managed_session() as sess:\n",
    "    for epoch in range(1, hp.num_epochs+1): \n",
    "        if sv.should_stop(): break\n",
    "        for step in tqdm.tqdm(range(g.num_batch), total=g.num_batch, ncols=70, leave=False, unit='b'):\n",
    "            sess.run(g.train_op)\n",
    "\n",
    "        gs = sess.run(g.global_step)   \n",
    "        sv.saver.save(sess, hp.logdir + '/model_epoch_%02d_gs_%d' % (epoch, gs))\n",
    "\n",
    "print(\"Done\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
